# Feature extraction parameters
feature:
  n_mels: 128  # Increased from 64 for better frequency resolution
  frames: 5
  n_fft: 1024
  hop_length: 512
  stride: 1  # Reduced from 2 for finer feature extraction
  power: 2.0
  sr: 16000
  augmentation:
    enabled: true
    max_mask_freq: 15  # Increased from 10 to create more varied training samples
    max_mask_time: 15  # Increased from 10 to create more varied training samples
    n_freq_masks: 2
    n_time_masks: 3  # Increased from 2 for better temporal masking
    noise_level: 0.01  # Increased from 0.005 for more robust training
  audio_augmentation:
    enabled: true
    time_stretch:
      enabled: true
      probability: 0.6  # Increased from 0.5
      min_factor: 0.7  # More aggressive time stretching
      max_factor: 1.3  # More aggressive time stretching
    pitch_shift:
      enabled: true
      probability: 0.6  # Increased from 0.5
      min_steps: -4  # More pitch variety
      max_steps: 4  # More pitch variety
    background_noise:
      enabled: true
      probability: 0.6  # Increased from 0.5
      min_factor: 0.005  # Increased from 0.001
      max_factor: 0.04  # Increased from 0.02
  preprocessing_batch_size: 64
  dataset_chunking:
    enabled: true
    chunk_size: 5000
    temp_directory: "./temp_chunks/temp_chunks_AST"

# Cache configuration for spectrogram generation
cache:
  enabled: true
  directory: "./cache/mast_spectrograms"
  memory_limit_mb: 4096
  persistent: true
  clear_on_param_change: true
  compression: true

# Fit algorithm parameters
fit:
  compile:
    optimizer: "adam"
    learning_rate: 0.0003  # Modified from 0.0001 - slightly higher learning rate
    loss: "focal_loss"  # Changed from "binary_crossentropy" for better handling of class imbalance
    metrics: ["accuracy", "Precision", "Recall", "AUC"]
    weighted_metrics: ["accuracy"]
  epochs: 50  # Increased from 30
  batch_size: 32  # Increased from 16 for better gradient estimation
  shuffle: True
  validation_split: 0.1
  verbose: 1
  early_stopping:
    enabled: true
    monitor: "val_loss"
    patience: 10  # Changed from 15
    min_delta: 0.001
    restore_best_weights: true
  weight_factor: 2.0  # Increased from 1.5 for non-problematic IDs
  weighted_machine_ids: []  # Will handle id_00 and id_04 explicitly in code
  apply_sample_weights: true  # Enable sample weighting
  special_case_weights: # Special case weights for problematic machine IDs
    fan_id_00: 2.5  # Increased from 2.0
    fan_id_04: 2.5  # Increased from 2.0
  lr_scheduler:
    enabled: true
    monitor: "val_loss"
    factor: 0.5  # Changed from 0.1 to less aggressive reduction
    patience: 5
    min_delta: 0.001
    cooldown: 1  # Reduced from 2
    min_lr: 0.00000001
  checkpointing:
    enabled: true
    monitor: "val_accuracy"
    mode: "max"
    save_best_only: true
  class_weight_balancing: true
  abnormal_weight_multiplier: 2.0  # Increased from 1.0
  default_abnormal_weight: 5.0  # Changed from 10.0 to avoid over-emphasizing
  warmup:
    enabled: true
    epochs: 3  # Reduced from 5
    hold_epochs: 1  # Increased from 0

# Training optimizations
training:
  find_optimal_lr: false
  mixed_precision: true
  xla_acceleration: false
  gradient_accumulation_steps: 2  # Increased from 1
  gradient_clip_norm: 1.0
  mixup:
    enabled: true
    alpha: 0.3  # Increased from 0.2 for more mixing
  checkpointing:
    save_frequency: 1000
    keep_checkpoints: 5
  memory_optimization:
    clear_memory_frequency: 50
    prefetch_buffer_size: 4

# Enhanced transformer model settings
model:
  architecture:
    transformer:
      num_heads: 8  # Increased from 4
      dim_feedforward: 768  # Increased from 512
      num_encoder_layers: 3  # Increased from 2
      positional_encoding: true
      use_pretrained: false  # Changed to false to avoid dependency issues
      patch_size: 4
      attention_dropout: 0.2  # Increased from 0.1
      attention_type: "standard"  # Changed from "efficient" 
      pos_encoding_type: "sinusoidal"
      layer_norm_epsilon: 1.0e-6
      activation_fn: "gelu"
      ff_dim_multiplier: 4
      enable_rotary: true  # Changed from false - better sequence modeling
  loss: "focal_loss"  # Changed from "binary_crossentropy"
  focal_loss:
    gamma: 2.5  # Increased from 2.0 - focuses more on hard examples
    alpha: 0.5  # Increased from 0.25 - balances class importance
    use_safe_implementation: true
  l2_regularization: 0.0001  # Increased from 0.00001

# MAST specific configuration
mast:
  pretraining:
    enabled: true
    epochs: 10
    batch_size: 32
    use_normal_only: true
    learning_rate: 1e-4
    masking:
      probability: 0.15
      mask_length: 8
      mask_time: true
      mask_freq: true
  finetuning:
    enabled: true
    initial_learning_rate: 5e-5
    warmup_epochs: 2
    high_contrast_activation: true
    contrast_factor: 5.0
  architecture:
    reconstruction_loss: "mse"
    two_phase_training: true
    trainable_positional_encodings: false
    patch_prediction: true
    use_cls_token: true

# Dataset generation parameters
dataset:
  split_ratio: [0.7, 0.1, 0.2]  # Changed from [0.8, 0.1, 0.1] - more test data
  file_extension: "wav"
  default_prediction: 0.5
  prediction_calibration: true  # New parameter for calibrating predictions

# Evaluation parameters
evaluation:
  custom_threshold: 0.5
  calibrate_predictions: true
  threshold_search:
    enabled: true
    range: [0.3, 0.7]
    step: 0.05
  metrics: ["accuracy", "precision", "recall", "f1_score", "auc"]

# Debug settings
debug:
  enabled: false  # Changed from true - use full dataset
  sample_size: 500

# Visualization parameters
visualization:
  figure_size: [30, 20]
  history_plots: ["loss", "accuracy", "precision", "recall"]

# Directory paths
base_directory: "./dataset"
pickle_directory: "./pickle/pickle_MAST"
model_directory: "./model/MAST"
result_directory: "./result/result_MAST"
result_file: "result_MAST.yaml"

# Logging parameters
logging:
  level: "DEBUG"
  file: "./logs/log_MAST/baseline_MAST.log"