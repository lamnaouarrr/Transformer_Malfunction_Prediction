########################################################################
# main
########################################################################
def main():
    # Set memory growth before any other TensorFlow operations
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        for device in physical_devices:
            try:
                # Enable memory growth for better memory management
                tf.config.experimental.set_memory_growth(device, True)
                logger.info(f"Enabled memory growth for {device}")
            except Exception as e:
                logger.warning(f"Could not set memory growth for {device}: {e}")
        
        # Verify GPU is being used
        logger.info(f"TensorFlow is using GPU: {tf.test.is_gpu_available()}")
        logger.info(f"Available GPUs: {tf.config.list_physical_devices('GPU')}")
    
    # Configure memory growth for V100 32GB
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # Allow TensorFlow to allocate memory as needed, but set a growth limit
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            # Set memory limit to 30GB (leaving some headroom)
            tf.config.experimental.set_virtual_device_configuration(
                gpus[0],
                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=30 * 1024)]
            )
            logger.info("GPU memory configuration set for V100 32GB")
        except RuntimeError as e:
            logger.error(f"Error setting GPU memory configuration: {e}")

    with open("baseline_AST.yaml", "r") as stream:
        param = yaml.safe_load(stream)    

    # Enable XLA compilation for faster GPU execution
    if param.get("training", {}).get("xla_acceleration", True):
        logger.info("Enabling XLA acceleration for faster training")
        try:
            tf.config.optimizer.set_jit(True)  # Enable XLA
            os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'  # Force XLA on all operations
            logger.info("XLA acceleration enabled successfully")
        except Exception as e:
            logger.warning(f"Failed to enable XLA acceleration: {e}")


    print("============== CHECKING DIRECTORY STRUCTURE ==============")
    normal_dir = Path(param["base_directory"]) / "normal"
    abnormal_dir = Path(param["base_directory"]) / "abnormal"

    print(f"Normal directory exists: {normal_dir.exists()}")
    if normal_dir.exists():
        normal_files = list(normal_dir.glob("*.wav"))
        print(f"Number of normal files found: {len(normal_files)}")
        if normal_files:
            print(f"Sample normal filename: {normal_files[0].name}")

    print(f"Abnormal directory exists: {abnormal_dir.exists()}")
    if abnormal_dir.exists():
        abnormal_files = list(abnormal_dir.glob("*.wav"))
        print(f"Number of abnormal files found: {len(abnormal_files)}")
        if abnormal_files:
            print(f"Sample abnormal filename: {abnormal_files[0].name}")

    start_time = time.time()

    with open("baseline_AST.yaml", "r") as stream:
        param = yaml.safe_load(stream)

    os.makedirs(param["pickle_directory"], exist_ok=True)
    os.makedirs(param["model_directory"], exist_ok=True)
    os.makedirs(param["result_directory"], exist_ok=True)

    visualizer = Visualizer(param)

    # Test audio file loading directly
    normal_path = Path(param["base_directory"]) / "normal"
    abnormal_path = Path(param["base_directory"]) / "abnormal"
    
    # Try to load a sample file directly
    test_files = list(normal_path.glob("*.wav"))[:1] if normal_path.exists() and list(normal_path.glob("*.wav")) else list(abnormal_path.glob("*.wav"))[:1]
    
    if test_files:
        print(f"DEBUG: Testing direct audio load for: {test_files[0]}")
        sr, y = demux_wav(str(test_files[0]))
        if y is not None:
            print(f"DEBUG: Successfully loaded audio with sr={sr}, length={len(y)}")
        else:
            print(f"DEBUG: Failed to load audio file")
    else:
        print("DEBUG: No test files found to verify audio loading")

    base_path = Path(param["base_directory"])

    print("============== COUNTING DATASET SAMPLES ==============")
    logger.info("Counting all samples in the dataset...")
    
    normal_path = Path(param["base_directory"]) / "normal"
    abnormal_path = Path(param["base_directory"]) / "abnormal"
    
    # Count files
    total_normal_files = len(list(normal_path.glob("*.wav"))) if normal_path.exists() else 0
    total_abnormal_files = len(list(abnormal_path.glob("*.wav"))) if abnormal_path.exists() else 0
    
    # Log the total counts
    logger.info(f"Total normal files in dataset: {total_normal_files}")
    logger.info(f"Total abnormal files in dataset: {total_abnormal_files}")
    logger.info(f"Total files in dataset: {total_normal_files + total_abnormal_files}")

    # Define target_dir (use normal_path as default, since dataset_generator will find corresponding abnormal files)
    target_dir = normal_path if normal_path.exists() else abnormal_path
    if not target_dir.exists():
        logger.error("Neither normal nor abnormal directory exists!")
        return

    # Get model file information from the first file in the directory
    sample_files = list(Path(target_dir).glob(f"*.{param.get('dataset', {}).get('file_extension', 'wav')}"))
    print(f"DEBUG: Found {len(sample_files)} files in {target_dir}")
    print(f"DEBUG: First 5 files: {[f.name for f in sample_files[:5]]}")

    if not sample_files:
        logger.warning(f"No files found in {target_dir}")
        return  # Exit main() if no files are found

    # Parse a sample filename to get db, machine_type, machine_id
    filename = sample_files[0].name
    parts = filename.split('_')
    print(f"DEBUG: Parsing filename '{filename}' into parts: {parts}")

    if len(parts) < 4:
        logger.warning(f"Filename format incorrect: {filename}")
        return  # Exit main() if filename format is incorrect
    
    # Use a straightforward key without unintended characters
    evaluation_result_key = "overall_model"
    print(f"DEBUG: Using evaluation_result_key: {evaluation_result_key}")

    # Initialize evaluation result dictionary
    evaluation_result = {}
    
    # Initialize results dictionary if it doesn't exist
    results = {}
    all_y_true = []
    all_y_pred = []
    result_file = f"{param['result_directory']}/result_AST.yaml"

    print("============== DATASET_GENERATOR ==============")
    train_pickle = f"{param['pickle_directory']}/train_overall.pickle"
    train_labels_pickle = f"{param['pickle_directory']}/train_labels_overall.pickle"
    val_pickle = f"{param['pickle_directory']}/val_overall.pickle"
    val_labels_pickle = f"{param['pickle_directory']}/val_labels_overall.pickle"
    test_files_pickle = f"{param['pickle_directory']}/test_files_overall.pickle"
    test_labels_pickle = f"{param['pickle_directory']}/test_labels_overall.pickle"

    # Initialize variables
    train_files, train_labels, val_files, val_labels, test_files, test_labels = [], [], [], [], [], []

    if (os.path.exists(train_pickle) and os.path.exists(train_labels_pickle) and
        os.path.exists(val_pickle) and os.path.exists(val_labels_pickle) and
        os.path.exists(test_files_pickle) and os.path.exists(test_labels_pickle)):
        train_data = load_pickle(train_pickle)
        train_labels = load_pickle(train_labels_pickle)
        val_data = load_pickle(val_pickle)
        val_labels = load_pickle(val_labels_pickle)
        test_files = load_pickle(test_files_pickle)
        test_labels = load_pickle(test_labels_pickle)
    else:
        train_files, train_labels, val_files, val_labels, test_files, test_labels = dataset_generator(target_dir, param=param)

        if len(train_files) == 0 or len(val_files) == 0 or len(test_files) == 0:
            logger.error(f"No files found for {evaluation_result_key}, skipping...")
            return  # Exit main() if no files are found after generation

    debug_mode = param.get("debug", {}).get("enabled", False)
    debug_sample_size = param.get("debug", {}).get("sample_size", 100)

    if debug_mode:
        logger.info(f"DEBUG MODE: Using small dataset with {debug_sample_size} samples")
        train_files, train_labels = create_small_dataset(train_files, train_labels, debug_sample_size)
        val_files, val_labels = create_small_dataset(val_files, val_labels, debug_sample_size // 2)
        test_files, test_labels = create_small_dataset(test_files, test_labels, debug_sample_size // 2)
        
        logger.info(f"DEBUG dataset sizes - Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}")


    preprocessing_batch_size = param.get("feature", {}).get("preprocessing_batch_size", 64)
    chunking_enabled = param.get("feature", {}).get("dataset_chunking", {}).get("enabled", False)
    chunk_size = param.get("feature", {}).get("dataset_chunking", {}).get("chunk_size", 5000)

    # For training data
    if chunking_enabled and len(train_files) > chunk_size:
        logger.info(f"Processing training data in chunks (dataset size: {len(train_files)} files)")
        train_data, train_labels_expanded = process_dataset_in_chunks(
            train_files,
            train_labels,
            chunk_size=chunk_size,
            param=param
        )
    else:
        train_data, train_labels_expanded = list_to_spectrograms(
            train_files,
            train_labels,
            msg="generate train_dataset",
            augment=True,
            param=param,
            batch_size=preprocessing_batch_size
        )

    # For validation data
    if chunking_enabled and len(val_files) > chunk_size:
        logger.info(f"Processing validation data in chunks (dataset size: {len(val_files)} files)")
        val_data, val_labels_expanded = process_dataset_in_chunks(
            val_files,
            val_labels,
            chunk_size=chunk_size,
            param=param
        )
    else:
        val_data, val_labels_expanded = list_to_spectrograms(
            val_files,
            val_labels,
            msg="generate validation_dataset",
            augment=False,
            param=param,
            batch_size=preprocessing_batch_size
        )

    # For test data
    if chunking_enabled and len(test_files) > chunk_size:
        logger.info(f"Processing test data in chunks (dataset size: {len(test_files)} files)")
        test_data, test_labels_expanded = process_dataset_in_chunks(
            test_files,
            test_labels,
            chunk_size=chunk_size,
            param=param
        )
    else:
        test_data, test_labels_expanded = list_to_spectrograms(
            test_files,
            test_labels,
            msg="generate test_dataset",
            augment=False,
            param=param,
            batch_size=preprocessing_batch_size
        )

    if train_data.shape[0] == 0 or val_data.shape[0] == 0:
        logger.error(f"No valid training/validation data for {evaluation_result_key}, skipping...")
        return  # Exit main() if no valid training/validation data

    logger.info("Applying robust standardization to spectrograms...")
    train_data = standardize_spectrograms(train_data)
    val_data = standardize_spectrograms(val_data)
    if 'test_data' in locals() and test_data is not None:
        test_data = standardize_spectrograms(test_data)

    save_pickle(train_pickle, train_data)
    save_pickle(train_labels_pickle, train_labels_expanded)
    save_pickle(val_pickle, val_data)
    save_pickle(val_labels_pickle, val_labels_expanded)
    save_pickle(test_files_pickle, test_files)
    save_pickle(test_labels_pickle, test_labels)

    # Print shapes
    logger.info(f"Training data shape: {train_data.shape}")
    logger.info(f"Training labels shape: {train_labels_expanded.shape}")
    logger.info(f"Validation data shape: {val_data.shape}")
    logger.info(f"Validation labels shape: {val_labels_expanded.shape}")
    logger.info(f"Number of test files: {len(test_files)}")

    # Define target shape for spectrograms
    target_shape = (param["feature"]["n_mels"], 96)
    logger.info(f"Target spectrogram shape: {target_shape}")

    # Preprocess to ensure consistent shapes
    logger.info("Preprocessing training data...")
    train_data = preprocess_spectrograms(train_data, target_shape)
    logger.info(f"Preprocessed train data shape: {train_data.shape}")

    logger.info("Preprocessing validation data...")
    val_data = preprocess_spectrograms(val_data, target_shape)
    logger.info(f"Preprocessed validation data shape: {val_data.shape}")

    # Normalize data for better training
    logger.info("Normalizing data...")
    # Calculate mean and std from training data
    train_mean = np.mean(train_data)
    train_std = np.std(train_data)
    logger.info(f"Training data statistics - Mean: {train_mean:.4f}, Std: {train_std:.4f}")

    # Apply normalization if std is not too small
    if train_std > 1e-6:
        train_data = (train_data - train_mean) / train_std
        val_data = (val_data - train_mean) / train_std
        logger.info("Z-score normalization applied")
    else:
        # If std is too small, just center the data
        train_data = train_data - train_mean
        val_data = val_data - train_mean
        logger.info("Mean centering applied (std too small for z-score)")

    # Balance the dataset
    logger.info("Balancing dataset...")
    train_data, train_labels_expanded = balance_dataset(train_data, train_labels_expanded, augment_minority=True)

    # Apply data augmentation
    augmented_data = []
    augmented_labels = []

    # Get abnormal samples
    abnormal_indices = np.where(train_labels_expanded == 1)[0]
    logger.info(f"Augmenting {len(abnormal_indices)} abnormal samples")

    # Augment each abnormal sample once with simple noise
    for idx in abnormal_indices:
        sample = train_data[idx].copy()
        noise = np.random.normal(0, 0.1, sample.shape)
        augmented_sample = sample + noise
        augmented_sample = np.clip(augmented_sample, 0, 1)
        
        augmented_data.append(augmented_sample)
        augmented_labels.append(1)  # Abnormal class

    # Add augmented samples to training data
    if augmented_data:
        augmented_data = np.array(augmented_data)
        train_data = np.vstack([train_data, augmented_data])
        train_labels_expanded = np.concatenate([train_labels_expanded, np.array(augmented_labels)])
        
        # Shuffle the combined dataset
        shuffle_indices = np.random.permutation(len(train_data))
        train_data = train_data[shuffle_indices]
        train_labels_expanded = train_labels_expanded[shuffle_indices]
        
        logger.info(f"After augmentation: {len(train_data)} samples, {np.sum(train_labels_expanded == 1)} abnormal")

    # Configure mixed precision
    mixed_precision_enabled = configure_mixed_precision(
        enabled=param.get("training", {}).get("mixed_precision", False)
    )

    # Check if we should use streaming data
    use_streaming = param.get("training", {}).get("streaming_data", {}).get("enabled", False)

    if use_streaming:
        logger.info("Using streaming data pipeline with tf.data")
        # Ensure all labels are float32
        if train_labels is not None:
            train_labels = np.array(train_labels, dtype=np.float32)
        if val_labels is not None:
            val_labels = np.array(val_labels, dtype=np.float32)
        if test_labels is not None:
            test_labels = np.array(test_labels, dtype=np.float32)
        
        # Create TensorFlow datasets
        batch_size = param.get("fit", {}).get("batch_size", 16)
        orig_batch_size = batch_size


        if batch_size < 32 and param.get("training", {}).get("optimize_batch_size", True):
            batch_size = 32  # Suggested size for V100 32GB
            logger.info(f"Increased batch size from {orig_batch_size} to {batch_size} for V100 32GB (can be disabled in config)")

        # Update the configuration to use this optimized batch size
        param["fit"]["batch_size"] = batch_size

        logger.info(f"Using batch size {batch_size} optimized for V100 32GB")
        train_dataset = create_tf_dataset(
            train_files, 
            train_labels, 
            batch_size=batch_size, 
            is_training=True, 
            param=param
        )
        
        val_dataset = create_tf_dataset(
            val_files, 
            val_labels, 
            batch_size=batch_size, 
            is_training=False, 
            param=param
        )

    monitor_gpu_usage()

    # Debug
    normal_count = sum(1 for label in train_labels_expanded if label == 0)
    abnormal_count = sum(1 for label in train_labels_expanded if label == 1)
    print(f"Training data composition: Normal={normal_count}, Abnormal={abnormal_count}")
    
    # Check for data shape mismatch and fix it
    if train_data.shape[0] != train_labels_expanded.shape[0]:
        logger.warning(f"Data shape mismatch! X: {train_data.shape[0]} samples, y: {train_labels_expanded.shape[0]} labels")
        
        if train_data.shape[0] > train_labels_expanded.shape[0]:
            # Too many features, need to reduce
            train_data = train_data[:train_labels_expanded.shape[0]]
            logger.info(f"Reduced X to match y: {train_data.shape}")
        else:
            # Too many labels, need to reduce
            train_labels_expanded = train_labels_expanded[:train_data.shape[0]]
            logger.info(f"Reduced y to match X: {train_labels_expanded.shape}")

    # Check class distribution
    class_distribution = np.bincount(train_labels_expanded.astype(int))
    logger.info(f"Class distribution in training data: {class_distribution}")
    if len(class_distribution) > 1:
        class_ratio = class_distribution[0] / class_distribution[1] if class_distribution[1] > 0 else float('inf')
        logger.info(f"Class ratio (normal:abnormal): {class_ratio:.2f}:1")
        
        if class_ratio > 10:
            logger.warning(f"Severe class imbalance detected! Consider using class weights or data augmentation.")

    batch_size = param.get("fit", {}).get("batch_size", 32)
    epochs = param.get("fit", {}).get("epochs", 100)
    base_learning_rate = param.get("fit", {}).get("compile", {}).get("learning_rate", 0.001)

    # Scale learning rate based on batch size
    learning_rate = get_scaled_learning_rate(base_learning_rate, batch_size)
    logger.info(f"Scaled learning rate from {base_learning_rate} to {learning_rate} for batch size {batch_size}")

    # Log the training parameters being used
    logger.info(f"Training with batch_size={batch_size}, epochs={epochs}, learning_rate={learning_rate}")

    # Set fixed class weights with higher weight for abnormal class
    class_weights = {
        0: 1.0,  # Normal class
        1: 2.0   # Abnormal class - ensure higher weight
    }
    # Convert keys to integers to avoid dtype issues
    class_weights = {int(k): float(v) for k, v in class_weights.items()}
    logger.info(f"Using fixed class weights: {class_weights}")

    # Ensure consistent data types before training
    logger.info(f"Train data type: {train_data.dtype}")
    logger.info(f"Train labels type: {train_labels_expanded.dtype}")

    # Convert to float32 if needed
    if train_data.dtype != np.float32:
        logger.info("Converting train_data to float32")
        train_data = train_data.astype(np.float32)
        
    if train_labels_expanded.dtype != np.float32:
        logger.info("Converting train_labels to float32")
        train_labels_expanded = train_labels_expanded.astype(np.float32)
        
    if val_data.dtype != np.float32:
        logger.info("Converting val_data to float32")
        val_data = val_data.astype(np.float32)
        
    if val_labels_expanded.dtype != np.float32:
        logger.info("Converting val_labels to float32")
        val_labels_expanded = val_labels_expanded.astype(np.float32)

    def verify_gpu_usage():
        """Verify that TensorFlow is properly using the GPU"""
        # Create a simple test tensor and operation
        with tf.device('/GPU:0'):
            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
            b = tf.constant([[1.0, 1.0], [1.0, 1.0]])
            c = tf.matmul(a, b)
        
        # Check if the operation was executed on GPU
        logger.info(f"Test tensor device: {c.device}")
        if 'GPU' in c.device:
            logger.info("✓ GPU is properly configured and being used")
        else:
            logger.warning("⚠ GPU is not being used for tensor operations!")
        
        return 'GPU' in c.device

    print("============== VERIFYING GPU USAGE ==============")
    is_gpu_working = verify_gpu_usage()
    if not is_gpu_working:
        logger.warning("GPU is not being used! Training will be slow.")
        return

    print("============== MODEL TRAINING ==============")
    # Track model training time specifically
    model_start_time = time.time()
    # Define model_file and history_img variables
    model_file = f"{param['model_directory']}/model_overall_ast.keras"
    history_img = f"{param['result_directory']}/history_overall_ast.png"

    # Enable mixed precision training
    if param.get("training", {}).get("mixed_precision", False):
        logger.info("Enabling mixed precision training")
        mixed_precision.set_global_policy('mixed_float16')
        logger.info(f"Mixed precision policy enabled: {mixed_precision.global_policy()}")

    # Check if we should use progressive training
    use_progressive = param.get("training", {}).get("progressive_training", {}).get("enabled", False)

    if use_progressive and not os.path.exists(model_file):
        logger.info("Using progressive training with increasing spectrogram sizes")
        
        # Check if we have valid training and validation files
        if not train_files or len(train_files) == 0:
            logger.error("No training files available for progressive training")
        elif not val_files or len(val_files) == 0:
            logger.warning("No validation files for progressive training, will use a portion of training data")
        else:
            model, progressive_history = implement_progressive_training(
                None,  # No initial model
                train_files,
                train_labels,
                val_files,
                val_labels,
                param
            )
            
            # Check if progressive training was successful
            if model is None:
                logger.error("Progressive training failed, falling back to standard training")
                use_progressive = False
            else:
                # Combine histories
                history = {
                    'history': {
                        'loss': [],
                        'accuracy': [],
                        'val_loss': [],
                        'val_accuracy': []
                    }
                }
                
                for h in progressive_history:
                    history['history']['loss'].extend(h.history['loss'])
                    history['history']['accuracy'].extend(h.history['accuracy'])
                    history['history']['val_loss'].extend(h.history['val_loss'])
                    history['history']['val_accuracy'].extend(h.history['val_accuracy'])
                
                # Convert to object with history attribute for compatibility
                history = type('History', (), history)
                
                # Save final model
                try:
                    model.save(model_file)
                    logger.info(f"Model saved to {model_file}")
                except Exception as e:
                    logger.warning(f"Error saving model: {e}")
                    try:
                        model.save(model_file.replace('.keras', ''))
                        logger.info(f"Model saved with alternative format to {model_file.replace('.keras', '')}")
                    except Exception as e2:
                        logger.error(f"All attempts to save model failed: {e2}")
    else:
        # Regular training without progressive resizing
        if os.path.exists(model_file) or os.path.exists(f"{model_file}.index"):
            try:
                # Try loading with different formats
                if os.path.exists(model_file):
                    model = tf.keras.models.load_model(model_file, custom_objects={"binary_cross_entropy_loss": binary_cross_entropy_loss})
                else:
                    model = tf.keras.models.load_model(f"{model_file}", custom_objects={"binary_cross_entropy_loss": binary_cross_entropy_loss})
                logger.info("Model loaded from file, no training performed")
            except Exception as e:
                logger.error(f"Error loading model: {e}")
                # Create a new model
                model = create_ast_model(
                    input_shape=(target_shape[0], target_shape[1]),
                    config=param.get("model", {}).get("architecture", {})
                )
                logger.info("Created new model due to loading error")
        else:
            # Define callbacks
            callbacks = []
            
            # Early stopping
            early_stopping_config = param.get("fit", {}).get("early_stopping", {})
            if early_stopping_config.get("enabled", False):
                callbacks.append(tf.keras.callbacks.EarlyStopping(
                    monitor=early_stopping_config.get("monitor", "val_loss"),
                    patience=20,
                    min_delta=early_stopping_config.get("min_delta", 0.001),
                    restore_best_weights=True,
                    verbose=1
                ))

            callbacks.append(TerminateOnNaN(patience=3))
            logger.info("Added NaN detection callback")
                    
            # Reduce learning rate on plateau
            lr_config = param.get("fit", {}).get("lr_scheduler", {})
            if lr_config.get("enabled", False):
                logger.info("Adding ReduceLROnPlateau callback with settings:")
                logger.info(f"  - Monitor: {lr_config.get('monitor', 'val_loss')}")
                logger.info(f"  - Factor: {lr_config.get('factor', 0.1)}")
                logger.info(f"  - Patience: {lr_config.get('patience', 5)}")
                callbacks.append(ReduceLROnPlateau(
                    monitor=lr_config.get("monitor", "val_loss"),
                    factor=lr_config.get("factor", 0.1),
                    patience=lr_config.get("patience", 5),
                    min_delta=lr_config.get("min_delta", 0.001),
                    cooldown=lr_config.get("cooldown", 2),
                    min_lr=lr_config.get("min_lr", 0.00000001),
                    verbose=1
                ))

            # Add learning rate scheduler with warmup
            callbacks.append(
                tf.keras.callbacks.LearningRateScheduler(
                    create_lr_schedule(initial_lr=0.001, warmup_epochs=5, decay_epochs=50)
                )
            )

            # Add callback to detect and handle NaN values
            callbacks.append(tf.keras.callbacks.TerminateOnNaN())
            
            # Create model with the correct input shape
            model = create_ast_model(
                input_shape=(target_shape[0], target_shape[1]),
                config=param.get("model", {}).get("architecture", {})
            )
            
            model_config = param.get("model", {}).get("architecture", {})
            model.summary()
            
            # Compile model
            compile_params = param["fit"]["compile"].copy()
            loss_type = param.get("model", {}).get("loss", "binary_crossentropy")
            
            # Handle learning_rate separately for the optimizer
            learning_rate = compile_params.pop("learning_rate", 0.0001)
            
            # Adjust optimizer for mixed precision if enabled
            clipnorm = param.get("training", {}).get("gradient_clip_norm", 1.0)
            if mixed_precision_enabled and compile_params.get("optimizer") == "AdamW":
                optimizer = AdamW(learning_rate=learning_rate, clipnorm=clipnorm)
                logger.info(f"Using AdamW optimizer with mixed precision and gradient clipping (clipnorm={clipnorm})")
            else:
                optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, clipnorm=clipnorm)
                logger.info(f"Using AdamW optimizer with gradient clipping (clipnorm={clipnorm})")

            # Use a more stable loss function
            if loss_type == "binary_crossentropy":
                loss_fn = "binary_crossentropy"  # Use TF's built-in implementation for stability
            elif loss_type == "focal_loss":
                # Only use focal loss if explicitly requested and with safeguards
                gamma = param.get("model", {}).get("focal_loss", {}).get("gamma", 2.0)
                alpha = param.get("model", {}).get("focal_loss", {}).get("alpha", 0.25)
                
                # Check if we should use a safer implementation
                use_safe_focal = param.get("model", {}).get("focal_loss", {}).get("use_safe_implementation", True)
                if use_safe_focal:
                    logger.info("Using numerically stable focal loss implementation")
                    loss_fn = lambda y_true, y_pred: focal_loss(y_true, y_pred, gamma, alpha)
                else:
                    logger.warning("Using standard focal loss - watch for NaN losses")
                    loss_fn = tf.keras.losses.BinaryFocalCrossentropy(
                        gamma=gamma, alpha=alpha, from_logits=False
                    )
            else:
                logger.info("Using standard binary crossentropy loss")
                loss_fn = "binary_crossentropy"
            
            #Define a custom loss function that combines binary cross-entropy with focal loss
            def combined_loss(y_true, y_pred):
                # Cast inputs to float32
                y_true = tf.cast(y_true, tf.float32)
                y_pred = tf.cast(y_pred, tf.float32)
                
                # Binary cross-entropy component
                bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
                
                # Add a penalty term that amplifies errors near the decision boundary
                boundary_penalty = 10.0 * y_true * (1 - y_pred) * (1 - y_pred) + 10.0 * (1 - y_true) * y_pred * y_pred
                
                return bce + tf.reduce_mean(boundary_penalty)

            # Use more aggressive class weights
            class_weights = {
                0: 1.0,  # Normal class
                1: 10.0   # Abnormal class - increase weight significantly
            }
            # Convert keys to integers to avoid dtype issues
            class_weights = {int(k): float(v) for k, v in class_weights.items()}

            model.compile(
                optimizer=optimizer,
                loss=combined_loss,
                metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]
            )
            
            # Check if we should use gradient accumulation
            if param.get("training", {}).get("gradient_accumulation_steps", 1) > 1:
                logger.info(f"Using gradient accumulation with {param['training']['gradient_accumulation_steps']} steps")
                
                # Get the dataset
                train_dataset = tf.data.Dataset.from_tensor_slices((train_data, tf.cast(train_labels_expanded, tf.float32)))
                train_dataset = train_dataset.batch(param["fit"]["batch_size"])
                
                # Create validation dataset
                val_dataset = tf.data.Dataset.from_tensor_slices((val_data, tf.cast(val_labels_expanded, tf.float32)))
                val_dataset = val_dataset.batch(param["fit"]["batch_size"])
                
                # Define variables to store accumulated gradients
                accum_steps = param["training"]["gradient_accumulation_steps"]
                
                # Create a history object to store metrics
                history_dict = {
                    'loss': [],
                    'accuracy': [],
                    'val_loss': [],
                    'val_accuracy': []
                }
                
                # Initialize accumulated gradients
                accumulated_gradients = [tf.Variable(tf.zeros_like(var), trainable=False) 
                                        for var in model.trainable_variables]
                
                # Training loop
                epochs = param["fit"]["epochs"]
                for epoch in range(epochs):
                    print(f"\nEpoch {epoch+1}/{epochs}")
                    
                    # Training metrics
                    train_loss = tf.keras.metrics.Mean()
                    train_accuracy = tf.keras.metrics.Mean()
                    
                    # Validation metrics
                    val_loss = tf.keras.metrics.Mean()
                    val_accuracy = tf.keras.metrics.Mean()
                    
                    # Training loop
                    step = 0
                    progress_bar = tqdm(train_dataset, desc=f"Training")
                    for x_batch, y_batch in progress_bar:
                        # Determine if this is the first batch in an accumulation cycle
                        first_batch = (step % accum_steps == 0)
                        
                        # Perform training step with the updated function
                        batch_loss, batch_accuracy = train_step_with_accumulation(
                            model, optimizer, loss_fn, x_batch, y_batch, accumulated_gradients, first_batch, accum_steps
                        )
                        
                        train_loss.update_state(batch_loss)
                        train_accuracy.update_state(batch_accuracy)
                        
                        # If we've accumulated enough gradients, apply them
                        if (step + 1) % accum_steps == 0 or (step + 1 == len(train_dataset)):
                            # Apply accumulated gradients
                            optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))
                            
                            # Log progress
                            progress_bar.set_postfix({
                                'loss': f'{train_loss.result():.4f}',
                                'accuracy': f'{train_accuracy.result():.4f}'
                            })
                        
                        step += 1
                        
                        # Clear memory periodically
                        if step % 50 == 0:
                            gc.collect()
                    
                    # Validation loop
                    for x_batch, y_batch in tqdm(val_dataset, desc=f"Validation"):
                        batch_loss, batch_accuracy = val_step(model, loss_fn, x_batch, y_batch)
                        val_loss.update_state(batch_loss)
                        val_accuracy.update_state(batch_accuracy)
                    
                    # Print epoch results
                    print(f"Training loss: {train_loss.result():.4f}, accuracy: {train_accuracy.result():.4f}")
                    print(f"Validation loss: {val_loss.result():.4f}, accuracy: {val_accuracy.result():.4f}")
                    
                    # Store metrics in history
                    history_dict['loss'].append(float(train_loss.result()))
                    history_dict['accuracy'].append(float(train_accuracy.result()))
                    history_dict['val_loss'].append(float(val_loss.result()))
                    history_dict['val_accuracy'].append(float(val_accuracy.result()))
                    
                    # Check for early stopping
                    if callbacks and any(isinstance(cb, tf.keras.callbacks.EarlyStopping) for cb in callbacks):
                        early_stopping_callback = next(cb for cb in callbacks if isinstance(cb, tf.keras.callbacks.EarlyStopping))
                        
                        # Get the monitored value
                        if early_stopping_callback.monitor == 'val_loss':
                            current = float(val_loss.result())
                        elif early_stopping_callback.monitor == 'val_accuracy':
                            current = float(val_accuracy.result())
                        
                        # Check if we should stop
                        if hasattr(early_stopping_callback, 'best') and early_stopping_callback.monitor == 'val_loss':
                            if early_stopping_callback.best is None or current < early_stopping_callback.best:
                                early_stopping_callback.best = current
                                early_stopping_callback.wait = 0
                                try:
                                    model.save(model_file)
                                    logger.info(f"Model saved to {model_file}")
                                except Exception as e:
                                    logger.warning(f"Error saving model: {e}")
                                    try:
                                        model.save(model_file.replace('.keras', ''))
                                        logger.info(f"Model saved with alternative format to {model_file.replace('.keras', '')}")
                                    except Exception as e2:
                                        logger.error(f"All attempts to save model failed: {e2}")
                                logger.info(f"Saved best model at epoch {epoch+1}")
                            else:
                                early_stopping_callback.wait += 1
                                if early_stopping_callback.wait >= early_stopping_callback.patience:
                                    print(f"Early stopping triggered at epoch {epoch+1}")
                                    break
                        elif hasattr(early_stopping_callback, 'best') and early_stopping_callback.monitor == 'val_accuracy':
                            if early_stopping_callback.best is None or current > early_stopping_callback.best:
                                early_stopping_callback.best = current
                                early_stopping_callback.wait = 0
                                try:
                                    model.save(model_file)
                                    logger.info(f"Model saved to {model_file}")
                                except Exception as e:
                                    logger.warning(f"Error saving model: {e}")
                                    try:
                                        model.save(model_file.replace('.keras', ''))
                                        logger.info(f"Model saved with alternative format to {model_file.replace('.keras', '')}")
                                    except Exception as e2:
                                        logger.error(f"All attempts to save model failed: {e2}")
                                logger.info(f"Saved best model at epoch {epoch+1}")
                            else:
                                early_stopping_callback.wait += 1
                                if early_stopping_callback.wait >= early_stopping_callback.patience:
                                    print(f"Early stopping triggered at epoch {epoch+1}")
                                    break
                    
                    # Save model periodically
                    if (epoch + 1) % 5 == 0:
                        try:
                            model.save(f"{param['model_directory']}/model_overall_ast_epoch_{epoch+1}.keras")
                            logger.info(f"Saved model checkpoint at epoch {epoch+1}")
                        except Exception as e:
                            logger.warning(f"Failed to save model checkpoint at epoch {epoch+1}: {e}")
                    
                    # Clear memory between epochs
                    gc.collect()
                
                # Convert history to a format compatible with Keras history
                history = type('History', (), {'history': history_dict})
            else:
                # Standard training without gradient accumulation
                if use_streaming:
                    # Train with streaming data
                    history = model.fit(
                        train_dataset,
                        epochs=param.get("fit", {}).get("epochs", 30),
                        validation_data=val_dataset,
                        callbacks=callbacks,
                        verbose=1
                    )
                else:
                    # Train with in-memory data
                    history = model.fit(
                        train_data,
                        train_labels_expanded,
                        batch_size=param.get("fit", {}).get("batch_size", 16),
                        epochs=param.get("fit", {}).get("epochs", 30),
                        validation_data=(val_data, val_labels_expanded),
                        class_weight=class_weights,
                        callbacks=callbacks,
                        verbose=1
                    )

    # Log training time
    training_time = time.time() - model_start_time
    logger.info(f"Model training completed in {training_time:.2f} seconds")

    # Make sure history exists before plotting
    if 'history' in locals():
        # Plot the training history
        visualizer.loss_plot(history)
        visualizer.save_figure(history_img)
    else:
        logger.warning("No training history available to plot")

    print("============== EVALUATION ==============")
    # Evaluate on test set
    test_data, test_labels_expanded = list_to_spectrograms(
        test_files,
        test_labels,
        msg="generate test_dataset",
        augment=False,
        param=param,
        batch_size=20
    )

    logger.info(f"Test data shape: {test_data.shape}")
    logger.info(f"Test labels shape: {test_labels_expanded.shape}")

    # Preprocess test data
    test_data = preprocess_spectrograms(test_data, target_shape)

    # Apply same normalization to test data
    if train_std > 1e-6:
        test_data = (test_data - train_mean) / train_std
    else:
        test_data = test_data - train_mean

    # Evaluate the model
    if test_data.shape[0] > 0:
        # Predict on test set
        y_pred = model.predict(test_data, batch_size=batch_size, verbose=1)
        
        # Now analyze the predictions (moved from above)
        logger.info(f"Raw prediction statistics:")
        logger.info(f"  - Min: {np.min(y_pred):.4f}, Max: {np.max(y_pred):.4f}")
        logger.info(f"  - Mean: {np.mean(y_pred):.4f}, Median: {np.median(y_pred):.4f}")
        
        # Count predictions in different ranges
        ranges = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
        for i in range(len(ranges)-1):
            count = np.sum((y_pred >= ranges[i]) & (y_pred < ranges[i+1]))
            logger.info(f"  - Predictions in range [{ranges[i]:.1f}, {ranges[i+1]:.1f}): {count} ({count/len(y_pred)*100:.1f}%)")
        
        # Plot histogram of predictions
        plt.figure(figsize=(10, 6))
        plt.hist(y_pred, bins=20)
        plt.title('Distribution of Raw Predictions')
        plt.xlabel('Prediction Value')
        plt.ylabel('Count')
        plt.savefig(f"{param['result_directory']}/prediction_distribution.png")
        plt.close()
        
        # Try multiple thresholds
        thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
        logger.info("Evaluating with multiple thresholds:")
        for thresh in thresholds:
            y_pred_binary_thresh = (y_pred > thresh).astype(int)
            accuracy = metrics.accuracy_score(test_labels_expanded, y_pred_binary_thresh)
            precision = metrics.precision_score(test_labels_expanded, y_pred_binary_thresh, zero_division=0)
            recall = metrics.recall_score(test_labels_expanded, y_pred_binary_thresh, zero_division=0)
            f1 = metrics.f1_score(test_labels_expanded, y_pred_binary_thresh, zero_division=0)
            logger.info(f"Threshold {thresh:.1f}: Acc={accuracy:.4f}, Prec={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}")
        
        # Apply standard threshold for final evaluation
        logger.info("Finding optimal classification threshold...")
        detection_threshold = find_optimal_threshold(test_labels_expanded, y_pred, param)
        logger.info(f"Using optimal detection threshold: {detection_threshold}")
        y_pred_binary = (y_pred > detection_threshold).astype(int)
        
        gc.collect()
        
        # Calculate metrics
        test_accuracy = metrics.accuracy_score(test_labels_expanded, y_pred_binary)
        test_precision = metrics.precision_score(test_labels_expanded, y_pred_binary, zero_division=0)
        test_recall = metrics.recall_score(test_labels_expanded, y_pred_binary, zero_division=0)
        test_f1 = metrics.f1_score(test_labels_expanded, y_pred_binary, zero_division=0)

        # Log metrics
        logger.info(f"Test Accuracy: {test_accuracy:.4f}")
        logger.info(f"Test Precision: {test_precision:.4f}")
        logger.info(f"Test Recall: {test_recall:.4f}")
        logger.info(f"Test F1 Score: {test_f1:.4f}")

        # Detailed classification report
        report = classification_report(
            test_labels_expanded, y_pred_binary, target_names=["Normal", "Abnormal"], zero_division=0
        )
        logger.info(f"Classification Report:\n{report}")

        # Plot confusion matrix
        cm_img = f"{param['result_directory']}/confusion_matrix_overall_ast.png"
        visualizer.plot_confusion_matrix(
            test_labels_expanded,
            y_pred_binary,
            title=f"Confusion Matrix (Overall)",
        )
        visualizer.save_figure(cm_img)

        # Store results
        evaluation_result = {
            "accuracy": float(test_accuracy),
            "precision": float(test_precision),
            "recall": float(test_recall),
            "f1": float(test_f1),
        }

        # Append to global results
        all_y_true.extend(test_labels_expanded)
        all_y_pred.extend(y_pred_binary)

    else:
        logger.warning("No test data available for evaluation")
        evaluation_result = {
            "accuracy": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
        }

    # Store evaluation results
    results[evaluation_result_key] = evaluation_result

    # Find optimal threshold using ROC curve
    if len(y_pred) > 0:
        logger.info("Finding optimal classification threshold using ROC curve...")
        
        # Calculate ROC curve
        fpr, tpr, thresholds = metrics.roc_curve(test_labels_expanded, y_pred)
        
        # Calculate the geometric mean of sensitivity and specificity
        gmeans = np.sqrt(tpr * (1-fpr))
        
        # Find the optimal threshold
        ix = np.argmax(gmeans)
        best_threshold = thresholds[ix]
        logger.info(f"Optimal threshold from ROC curve: {best_threshold:.4f}")
        logger.info(f"At this threshold - TPR: {tpr[ix]:.4f}, FPR: {fpr[ix]:.4f}, G-Mean: {gmeans[ix]:.4f}")
        
        # Re-evaluate with optimal threshold
        y_pred_binary = (y_pred > best_threshold).astype(int)

        # If optimal threshold is very close to 0.5, try a lower threshold
        if 0.45 < best_threshold < 0.55 and test_f1 < 0.6:
            logger.info("Trying a lower threshold (0.39) since optimal threshold is close to default")
            lower_threshold = 0.39
            y_pred_binary_lower = (y_pred > lower_threshold).astype(int)
            
            # Calculate metrics with lower threshold
            test_accuracy_lower = metrics.accuracy_score(test_labels_expanded, y_pred_binary_lower)
            test_precision_lower = metrics.precision_score(test_labels_expanded, y_pred_binary_lower, zero_division=0)
            test_recall_lower = metrics.recall_score(test_labels_expanded, y_pred_binary_lower, zero_division=0)
            test_f1_lower = metrics.f1_score(test_labels_expanded, y_pred_binary_lower, zero_division=0)
            
            logger.info(f"Metrics with lower threshold ({lower_threshold}):")
            logger.info(f"Test Accuracy: {test_accuracy_lower:.4f}")
            logger.info(f"Test Precision: {test_precision_lower:.4f}")
            logger.info(f"Test Recall: {test_recall_lower:.4f}")
            logger.info(f"Test F1 Score: {test_f1_lower:.4f}")
            
            # Use lower threshold if it gives better F1 score
            if test_f1_lower > test_f1:
                logger.info(f"Using lower threshold {lower_threshold} instead of optimal threshold {best_threshold}")
                y_pred_binary = y_pred_binary_lower
                test_accuracy = test_accuracy_lower
                test_precision = test_precision_lower
                test_recall = test_recall_lower
                test_f1 = test_f1_lower

    # Calculate overall metrics
    if all_y_true and all_y_pred:
        overall_accuracy = metrics.accuracy_score(all_y_true, all_y_pred)
        overall_precision = metrics.precision_score(all_y_true, all_y_pred, zero_division=0)
        overall_recall = metrics.recall_score(all_y_true, all_y_pred, zero_division=0)
        overall_f1 = metrics.f1_score(all_y_true, all_y_pred, zero_division=0)

        overall_results = {
            "overall_accuracy": float(overall_accuracy),
            "overall_precision": float(overall_precision),
            "overall_recall": float(overall_recall),
            "overall_f1": float(overall_f1),
        }
        results.update(overall_results)

        logger.info(f"Overall Accuracy: {overall_accuracy:.4f}")
        logger.info(f"Overall Precision: {overall_precision:.4f}")
        logger.info(f"Overall Recall: {overall_recall:.4f}")
        logger.info(f"Overall F1 Score: {overall_f1:.4f}")

        # Plot overall confusion matrix
        overall_cm_img = f"{param['result_directory']}/confusion_matrix_overall_ast.png"
        visualizer.plot_confusion_matrix(
            all_y_true,
            all_y_pred,
            title="Overall Confusion Matrix",
        )
        visualizer.save_figure(overall_cm_img)

    total_time = time.time() - start_time
    results["timing"] = {
        "total_execution_time_seconds": float(total_time),
        "model_training_time_seconds": float(training_time),
    }

    # Save results to YAML
    with open(result_file, "w") as f:
        yaml.safe_dump(results, f)
    logger.info(f"Results saved to {result_file}")

    # Log total execution time
    logger.info(f"Total execution time: {total_time:.2f} seconds")

if __name__ == "__main__":
    main()